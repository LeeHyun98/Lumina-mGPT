{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "ChameleonForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9419ae0bf1e545cc96453a8685c8a16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ChameleonForConditionalGeneration were not initialized from the model checkpoint at Alpha-VLLM/Lumina-mGPT-7B-768 and are newly initialized: ['model.vqmodel.encoder.conv_in.bias', 'model.vqmodel.encoder.conv_in.weight', 'model.vqmodel.encoder.conv_out.bias', 'model.vqmodel.encoder.conv_out.weight', 'model.vqmodel.encoder.down.0.block.0.conv1.bias', 'model.vqmodel.encoder.down.0.block.0.conv1.weight', 'model.vqmodel.encoder.down.0.block.0.conv2.bias', 'model.vqmodel.encoder.down.0.block.0.conv2.weight', 'model.vqmodel.encoder.down.0.block.0.norm1.bias', 'model.vqmodel.encoder.down.0.block.0.norm1.weight', 'model.vqmodel.encoder.down.0.block.0.norm2.bias', 'model.vqmodel.encoder.down.0.block.0.norm2.weight', 'model.vqmodel.encoder.down.0.block.1.conv1.bias', 'model.vqmodel.encoder.down.0.block.1.conv1.weight', 'model.vqmodel.encoder.down.0.block.1.conv2.bias', 'model.vqmodel.encoder.down.0.block.1.conv2.weight', 'model.vqmodel.encoder.down.0.block.1.norm1.bias', 'model.vqmodel.encoder.down.0.block.1.norm1.weight', 'model.vqmodel.encoder.down.0.block.1.norm2.bias', 'model.vqmodel.encoder.down.0.block.1.norm2.weight', 'model.vqmodel.encoder.down.0.downsample.conv.bias', 'model.vqmodel.encoder.down.0.downsample.conv.weight', 'model.vqmodel.encoder.down.1.block.0.conv1.bias', 'model.vqmodel.encoder.down.1.block.0.conv1.weight', 'model.vqmodel.encoder.down.1.block.0.conv2.bias', 'model.vqmodel.encoder.down.1.block.0.conv2.weight', 'model.vqmodel.encoder.down.1.block.0.norm1.bias', 'model.vqmodel.encoder.down.1.block.0.norm1.weight', 'model.vqmodel.encoder.down.1.block.0.norm2.bias', 'model.vqmodel.encoder.down.1.block.0.norm2.weight', 'model.vqmodel.encoder.down.1.block.1.conv1.bias', 'model.vqmodel.encoder.down.1.block.1.conv1.weight', 'model.vqmodel.encoder.down.1.block.1.conv2.bias', 'model.vqmodel.encoder.down.1.block.1.conv2.weight', 'model.vqmodel.encoder.down.1.block.1.norm1.bias', 'model.vqmodel.encoder.down.1.block.1.norm1.weight', 'model.vqmodel.encoder.down.1.block.1.norm2.bias', 'model.vqmodel.encoder.down.1.block.1.norm2.weight', 'model.vqmodel.encoder.down.1.downsample.conv.bias', 'model.vqmodel.encoder.down.1.downsample.conv.weight', 'model.vqmodel.encoder.down.2.block.0.conv1.bias', 'model.vqmodel.encoder.down.2.block.0.conv1.weight', 'model.vqmodel.encoder.down.2.block.0.conv2.bias', 'model.vqmodel.encoder.down.2.block.0.conv2.weight', 'model.vqmodel.encoder.down.2.block.0.nin_shortcut.bias', 'model.vqmodel.encoder.down.2.block.0.nin_shortcut.weight', 'model.vqmodel.encoder.down.2.block.0.norm1.bias', 'model.vqmodel.encoder.down.2.block.0.norm1.weight', 'model.vqmodel.encoder.down.2.block.0.norm2.bias', 'model.vqmodel.encoder.down.2.block.0.norm2.weight', 'model.vqmodel.encoder.down.2.block.1.conv1.bias', 'model.vqmodel.encoder.down.2.block.1.conv1.weight', 'model.vqmodel.encoder.down.2.block.1.conv2.bias', 'model.vqmodel.encoder.down.2.block.1.conv2.weight', 'model.vqmodel.encoder.down.2.block.1.norm1.bias', 'model.vqmodel.encoder.down.2.block.1.norm1.weight', 'model.vqmodel.encoder.down.2.block.1.norm2.bias', 'model.vqmodel.encoder.down.2.block.1.norm2.weight', 'model.vqmodel.encoder.down.2.downsample.conv.bias', 'model.vqmodel.encoder.down.2.downsample.conv.weight', 'model.vqmodel.encoder.down.3.block.0.conv1.bias', 'model.vqmodel.encoder.down.3.block.0.conv1.weight', 'model.vqmodel.encoder.down.3.block.0.conv2.bias', 'model.vqmodel.encoder.down.3.block.0.conv2.weight', 'model.vqmodel.encoder.down.3.block.0.norm1.bias', 'model.vqmodel.encoder.down.3.block.0.norm1.weight', 'model.vqmodel.encoder.down.3.block.0.norm2.bias', 'model.vqmodel.encoder.down.3.block.0.norm2.weight', 'model.vqmodel.encoder.down.3.block.1.conv1.bias', 'model.vqmodel.encoder.down.3.block.1.conv1.weight', 'model.vqmodel.encoder.down.3.block.1.conv2.bias', 'model.vqmodel.encoder.down.3.block.1.conv2.weight', 'model.vqmodel.encoder.down.3.block.1.norm1.bias', 'model.vqmodel.encoder.down.3.block.1.norm1.weight', 'model.vqmodel.encoder.down.3.block.1.norm2.bias', 'model.vqmodel.encoder.down.3.block.1.norm2.weight', 'model.vqmodel.encoder.down.3.downsample.conv.bias', 'model.vqmodel.encoder.down.3.downsample.conv.weight', 'model.vqmodel.encoder.down.4.block.0.conv1.bias', 'model.vqmodel.encoder.down.4.block.0.conv1.weight', 'model.vqmodel.encoder.down.4.block.0.conv2.bias', 'model.vqmodel.encoder.down.4.block.0.conv2.weight', 'model.vqmodel.encoder.down.4.block.0.nin_shortcut.bias', 'model.vqmodel.encoder.down.4.block.0.nin_shortcut.weight', 'model.vqmodel.encoder.down.4.block.0.norm1.bias', 'model.vqmodel.encoder.down.4.block.0.norm1.weight', 'model.vqmodel.encoder.down.4.block.0.norm2.bias', 'model.vqmodel.encoder.down.4.block.0.norm2.weight', 'model.vqmodel.encoder.down.4.block.1.conv1.bias', 'model.vqmodel.encoder.down.4.block.1.conv1.weight', 'model.vqmodel.encoder.down.4.block.1.conv2.bias', 'model.vqmodel.encoder.down.4.block.1.conv2.weight', 'model.vqmodel.encoder.down.4.block.1.norm1.bias', 'model.vqmodel.encoder.down.4.block.1.norm1.weight', 'model.vqmodel.encoder.down.4.block.1.norm2.bias', 'model.vqmodel.encoder.down.4.block.1.norm2.weight', 'model.vqmodel.encoder.mid.attn_1.k.bias', 'model.vqmodel.encoder.mid.attn_1.k.weight', 'model.vqmodel.encoder.mid.attn_1.norm.bias', 'model.vqmodel.encoder.mid.attn_1.norm.weight', 'model.vqmodel.encoder.mid.attn_1.proj_out.bias', 'model.vqmodel.encoder.mid.attn_1.proj_out.weight', 'model.vqmodel.encoder.mid.attn_1.q.bias', 'model.vqmodel.encoder.mid.attn_1.q.weight', 'model.vqmodel.encoder.mid.attn_1.v.bias', 'model.vqmodel.encoder.mid.attn_1.v.weight', 'model.vqmodel.encoder.mid.block_1.conv1.bias', 'model.vqmodel.encoder.mid.block_1.conv1.weight', 'model.vqmodel.encoder.mid.block_1.conv2.bias', 'model.vqmodel.encoder.mid.block_1.conv2.weight', 'model.vqmodel.encoder.mid.block_1.norm1.bias', 'model.vqmodel.encoder.mid.block_1.norm1.weight', 'model.vqmodel.encoder.mid.block_1.norm2.bias', 'model.vqmodel.encoder.mid.block_1.norm2.weight', 'model.vqmodel.encoder.mid.block_2.conv1.bias', 'model.vqmodel.encoder.mid.block_2.conv1.weight', 'model.vqmodel.encoder.mid.block_2.conv2.bias', 'model.vqmodel.encoder.mid.block_2.conv2.weight', 'model.vqmodel.encoder.mid.block_2.norm1.bias', 'model.vqmodel.encoder.mid.block_2.norm1.weight', 'model.vqmodel.encoder.mid.block_2.norm2.bias', 'model.vqmodel.encoder.mid.block_2.norm2.weight', 'model.vqmodel.encoder.norm_out.bias', 'model.vqmodel.encoder.norm_out.weight', 'model.vqmodel.post_quant_conv.bias', 'model.vqmodel.post_quant_conv.weight', 'model.vqmodel.quant_conv.bias', 'model.vqmodel.quant_conv.weight', 'model.vqmodel.quantize.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print PWD /data/projects/leehyun/Generative/Lumina-mGPT\n",
      "Project Root /data/projects/leehyun/Generative/Lumina-mGPT\n",
      "VQModel loaded from /data/projects/leehyun/Generative/Lumina-mGPT/lumina_mgpt/ckpts/chameleon/tokenizer/vqgan.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from lumina_mgpt.inference_solver import FlexARInferenceSolver\n",
    "\n",
    "# ******************** Image Generation ********************\n",
    "inference_solver = FlexARInferenceSolver(\n",
    "    model_path=\"Alpha-VLLM/Lumina-mGPT-7B-768\",\n",
    "    precision=\"bf16\",\n",
    "    target_size=768,\n",
    ")\n",
    "\n",
    "animal = \"bird\"\n",
    "q1 = f\"Generate an image of 768x768 according to the following prompt:\\nImage of a {animal} playing water, and a waterfall is in the background.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated: tuple of (generated response, list of generated images)\n",
    "generated = inference_solver.generate(\n",
    "    images=[],\n",
    "    qas=[[q1, None]],\n",
    "    max_gen_len=8192,\n",
    "    temperature=1.0,\n",
    "    logits_processor=inference_solver.create_logits_processor(\n",
    "        cfg=4.0, image_top_k=2000\n",
    "    ),\n",
    ")\n",
    "a1, new_image = generated[0], generated[1][0]\n",
    "\n",
    "now = datetime.now()\n",
    "save_path = f\"output_Alpha-VLLM/{animal}_{a1}.png\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "new_image.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* Image Understanding ******************\n",
    "inference_solver = FlexARInferenceSolver(\n",
    "    model_path=\"Alpha-VLLM/Lumina-mGPT-7B-512\",\n",
    "    precision=\"bf16\",\n",
    "    target_size=512,\n",
    ")\n",
    "\n",
    "# \"<|image|>\" symbol will be replaced with sequence of image tokens before fed to LLM\n",
    "q1 = \"Describe the image in detail. <|image|>\"\n",
    "\n",
    "images = [Image.open(save_path)]\n",
    "qas = [[q1, None]]\n",
    "\n",
    "# `len(images)` should be equal to the number of appearance of \"<|image|>\" in qas\n",
    "generated = inference_solver.generate(\n",
    "    images=images,\n",
    "    qas=qas,\n",
    "    max_gen_len=8192,\n",
    "    temperature=1.0,\n",
    "    logits_processor=inference_solver.create_logits_processor(\n",
    "        cfg=4.0, image_top_k=2000\n",
    "    ),\n",
    ")\n",
    "\n",
    "a1 = generated[0]\n",
    "\n",
    "print(a1)\n",
    "# save the generated text to a file\n",
    "with open(f\"{save_path}_description.txt\", \"w\") as f:\n",
    "    f.write(a1)\n",
    "\n",
    "\n",
    "# generated[1], namely the list of newly generated images, should typically be empty in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "ChameleonForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946e42c573cf49f69549dd907131dd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlumina_mgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_solver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlexARInferenceSolver\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#save_path = f\"output_Alzpha-VLLM/{animal}_{a1}.png\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ********************* Omni-Potent *********************\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m inference_solver \u001b[38;5;241m=\u001b[39m \u001b[43mFlexARInferenceSolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlpha-VLLM/Lumina-mGPT-7B-768-Omni\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbf16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Example: Depth Estimation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# For more instructions, see demos/demo_image2image.py\u001b[39;00m\n\u001b[1;32m     17\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSemantic segmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/data/projects/leehyun/Generative/Lumina-mGPT/lumina_mgpt/inference_solver.py:324\u001b[0m, in \u001b[0;36mFlexARInferenceSolver.__init__\u001b[0;34m(self, model_path, precision, target_size)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path, precision, target_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp32\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m    322\u001b[0m     }[precision]\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mChameleonForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_processor \u001b[38;5;241m=\u001b[39m FlexARItemProcessor(target_size\u001b[38;5;241m=\u001b[39mtarget_size)\n",
      "File \u001b[0;32m~/.conda/envs/Lumina-mGPT/lib/python3.10/site-packages/transformers/modeling_utils.py:4225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4216\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4218\u001b[0m     (\n\u001b[1;32m   4219\u001b[0m         model,\n\u001b[1;32m   4220\u001b[0m         missing_keys,\n\u001b[1;32m   4221\u001b[0m         unexpected_keys,\n\u001b[1;32m   4222\u001b[0m         mismatched_keys,\n\u001b[1;32m   4223\u001b[0m         offload_index,\n\u001b[1;32m   4224\u001b[0m         error_msgs,\n\u001b[0;32m-> 4225\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4246\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.conda/envs/Lumina-mGPT/lib/python3.10/site-packages/transformers/modeling_utils.py:4728\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4724\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4725\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4726\u001b[0m                 )\n\u001b[1;32m   4727\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4728\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4732\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4734\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4735\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4742\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4744\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4746\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/Lumina-mGPT/lib/python3.10/site-packages/transformers/modeling_utils.py:993\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    990\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/.conda/envs/Lumina-mGPT/lib/python3.10/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from lumina_mgpt.inference_solver import FlexARInferenceSolver\n",
    "#save_path = f\"output_Alzpha-VLLM/{animal}_{a1}.png\"\n",
    "# ********************* Omni-Potent *********************\n",
    "inference_solver = FlexARInferenceSolver(\n",
    "    model_path=\"Alpha-VLLM/Lumina-mGPT-7B-768-Omni\",\n",
    "    precision=\"bf16\",\n",
    "    target_size=768,\n",
    ")\n",
    "\n",
    "# Example: Depth Estimation\n",
    "# For more instructions, see demos/demo_image2image.py\n",
    "task = \"Semantic segmentation\"\n",
    "q1 = f\"{task}. <|image|>\"\n",
    "images = [Image.open(save_path)]\n",
    "qas = [[q1, None]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated = inference_solver.generate(\n",
    "    images=images,\n",
    "    qas=qas,\n",
    "    max_gen_len=8192,\n",
    "    temperature=1.0,\n",
    "    logits_processor=inference_solver.create_logits_processor(cfg=1.0, image_top_k=200),\n",
    ")\n",
    "\n",
    "a1 = generated[0]\n",
    "new_image = generated[1][0]\n",
    "\n",
    "\n",
    "save_path = f\"output_Alpha-VLLM/{task}_{animal}_{a1}.png\"\n",
    "new_image.save(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
